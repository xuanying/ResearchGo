$(function(){
    //数据处理
    var paper_info = $("#paper_rec").html();
    // paper_info = '[{"Paper_Title":"Disentangled Representation Learning GAN for Pose-Invariant Face Recognition","Paper_Journal":"2017 IEEE Conference on Computer Vision and Pattern Recognition","Journal_Level":"A","Paper_Year":"2017","Paper_Author":{"Luan Tran":{"Author_Email":"None","Author_Name":"Luan Tran","Author_ID":"None","Author_Level":"R1"},"Xi Yin":{"Author_Email":"None","Author_Name":"Xi Yin","Author_ID":"None","Author_Level":"R1"},"Xiaoming Liu":{"Author_Email":"None","Author_Name":"Xiaoming Liu","Author_ID":"None","Author_Level":"R1"}},"Paper_Abstract":"The large pose discrepancy between two face images is one of the key challenges in face recognition. Conventional approaches for pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator allows DR-GAN to learn a generative and discriminative representation, in addition to image synthesis. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified representation along with an arbitrary number of synthetic images. Quantitative and qualitative evaluation on both controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art.","Rel_Keyword":[{"Keyword":"Face Recognition","Keyword_Domain":"None","Rel_Level":"strong"},{"Keyword":"Generative Adversarial Network","Keyword_Domain":"None","Rel_Level":"strong"}],"Rel_Friend":{"Kihyuk Sohn":{"Author_Email":"None","Author_Name":"Kihyuk Sohn","Author_ID":"None","Rel_Level":"medium"},"Ha A. Le":{"Author_Email":"None","Author_Name":"Ha A. Le","Author_ID":"None","Rel_Level":"medium"}}},{"Paper_Title":"Understanding deep learning requires rethinking generalization","Paper_Journal":"ArXiv","Journal_Level":"A","Paper_Year":"2016","Paper_Author":{"Chiyuan Zhang":{"Author_Email":"None","Author_Name":"Chiyuan Zhang","Author_ID":"None","Author_Level":"R1"},"Samy Bengio":{"Author_Email":"None","Author_Name":"Samy Bengio","Author_ID":"None","Author_Level":"R1"},"Moritz Hardt":{"Author_Email":"None","Author_Name":"Moritz Hardt","Author_ID":"None","Author_Level":"R1"}},"Paper_Abstract":"Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.","Rel_Keyword":[{"Keyword":"Neural Networks","Keyword_Domain":"None","Rel_Level":"weak"},{"Keyword":"Deep Learning","Keyword_Domain":"None","Rel_Level":"medium"}],"Rel_Friend":{"Benjamin Recht":{"Author_Email":"None","Author_Name":"Benjamin Recht","Author_ID":"None","Rel_Level":"medium"},"Oriol Vinyals":{"Author_Email":"None","Author_Name":"Oriol Vinyals","Author_ID":"None","Rel_Level":"medium"}}},{"Paper_Title":"Deep Learning","Paper_Journal":"Adaptive computation and machine learning","Journal_Level":"A","Paper_Year":"2015","Paper_Author":{"Ian J. Goodfellow":{"Author_Email":"None","Author_Name":"Ian J. Goodfellow","Author_ID":"None","Author_Level":"R1"},"Peter Johnson":{"Author_Email":"None","Author_Name":"Peter Johnson","Author_ID":"None","Author_Level":"R1"},"Yoshua Bengio":{"Author_Email":"None","Author_Name":"Yoshua Bengio","Author_ID":"None","Author_Level":"R1"}},"Paper_Abstract":"Overview of the tutorial • A brief history of deep learning. • How to learn multi-layer generative models of unlabelled data by learning one layer of features at a time. – What is really going on when we stack RBMs to form a deep belief net. • How to use generative models to make discriminative training methods work much better for classification and regression. • How to modify RBMs to deal with real-valued input.","Rel_Keyword":[{"Keyword":"Deep Neural Network","Keyword_Domain":"None","Rel_Level":"weak"},{"Keyword":"Supervised","Keyword_Domain":"None","Rel_Level":"medium"}],"Rel_Friend":{"Yann Lecun":{"Author_Email":"None","Author_Name":"Yann Lecun","Author_ID":"None","Rel_Level":"medium"},"Yee Whye Teh":{"Author_Email":"None","Author_Name":"Yee Whye Teh","Author_ID":"None","Rel_Level":"medium"}}},{"Paper_Title":"Multimodal Deep Learning","Paper_Journal":"ICML","Journal_Level":"A","Paper_Year":"2011","Paper_Author":{"Andrew Y. Ng":{"Author_Email":"None","Author_Name":"Andrew Y. Ng","Author_ID":"None","Author_Level":"R1"},"Honglak Lee":{"Author_Email":"None","Author_Name":"Honglak Lee","Author_ID":"None","Author_Level":"R1"},"Juhan Nam":{"Author_Email":"None","Author_Name":"Juhan Nam","Author_ID":"None","Author_Level":"R1"}},"Paper_Abstract":"Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning.","Rel_Keyword":[{"Keyword":"Deep networks","Keyword_Domain":"None","Rel_Level":"weak"},{"Keyword":"Autoencoder","Keyword_Domain":"None","Rel_Level":"medium"}],"Rel_Friend":{"Mingyu Kim":{"Author_Email":"None","Author_Name":"Mingyu Kim","Author_ID":"None","Rel_Level":"medium"},"Aditya Khosla":{"Author_Email":"None","Author_Name":"Aditya Khosla","Author_ID":"None","Rel_Level":"medium"}}},{"Paper_Title":"A Fast Learning Algorithm for Deep Belief Nets","Paper_Journal":"Neural Computation","Journal_Level":"A","Paper_Year":"2006","Paper_Author":{"Yee Whye Teh":{"Author_Email":"None","Author_Name":"Yee Whye Teh","Author_ID":"None","Author_Level":"R1"},"Simon Osindero":{"Author_Email":"None","Author_Name":"Simon Osindero","Author_ID":"None","Author_Level":"R1"},"Geoffrey E. Hinton,":{"Author_Email":"None","Author_Name":"Geoffrey E. Hinton,","Author_ID":"None","Author_Level":"R1"}},"Paper_Abstract":"We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.","Rel_Keyword":[{"Keyword":"Boltzmann Machines RBMs","Keyword_Domain":"None","Rel_Level":"weak"},{"Keyword":"Deep Belief Networks DBN","Keyword_Domain":"None","Rel_Level":"medium"}],"Rel_Friend":{"Tapani Raiko":{"Author_Email":"None","Author_Name":"Tapani Raiko","Author_ID":"None","Rel_Level":"medium"},"Yann LeCun":{"Author_Email":"None","Author_Name":"Yann LeCun","Author_ID":"None","Rel_Level":"medium"}}}]';
    paper_info = '[{"Paper_Title":"Disentangled Representation Learning GAN for Pose-Invariant Face Recognition","Paper_Journal":"2017 IEEE Conference on Computer Vision and Pattern Recognition","Journal_Level":"A","Paper_Year":"2017","Paper_Author":{"Luan Tran":{"Author_Email":"None","Author_Name":"Luan Tran","Author_ID":"None","Author_Level":"R1"},"Xi Yin":{"Author_Email":"None","Author_Name":"Xi Yin","Author_ID":"None","Author_Level":"R1"},"Xiaoming Liu":{"Author_Email":"None","Author_Name":"Xiaoming Liu","Author_ID":"None","Author_Level":"R1"}},"Paper_Abstract":"The large pose discrepancy between two face images is one of the key challenges in face recognition. Conventional approaches for pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator allows DR-GAN to learn a generative and discriminative representation, in addition to image synthesis. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified representation along with an arbitrary number of synthetic images. Quantitative and qualitative evaluation on both controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art.","Rel_Keyword":[{"Keyword":"Face Recognition","Keyword_Domain":"None","Rel_Level":"strong"},{"Keyword":"Generative Adversarial Network","Keyword_Domain":"None","Rel_Level":"strong"}],"Rel_Friend":{"Kihyuk Sohn":{"Author_Email":"None","Author_Name":"Kihyuk Sohn","Author_ID":"None","Rel_Level":"medium"},"Ha A. Le":{"Author_Email":"None","Author_Name":"Ha A. Le","Author_ID":"None","Rel_Level":"medium"}}},{"Paper_Title":"Understanding deep learning requires rethinking generalization","Paper_Journal":"ArXiv","Journal_Level":"A","Paper_Year":"2016","Paper_Author":{"Chiyuan Zhang":{"Author_Email":"None","Author_Name":"Chiyuan Zhang","Author_ID":"None","Author_Level":"R1"},"Samy Bengio":{"Author_Email":"None","Author_Name":"Samy Bengio","Author_ID":"None","Author_Level":"R1"},"Moritz Hardt":{"Author_Email":"None","Author_Name":"Moritz Hardt","Author_ID":"None","Author_Level":"R1"}},"Paper_Abstract":"Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.","Rel_Keyword":[{"Keyword":"Neural Networks","Keyword_Domain":"None","Rel_Level":"weak"},{"Keyword":"Deep Learning","Keyword_Domain":"None","Rel_Level":"medium"}],"Rel_Friend":{"Benjamin Recht":{"Author_Email":"None","Author_Name":"Benjamin Recht","Author_ID":"None","Rel_Level":"medium"},"Oriol Vinyals":{"Author_Email":"None","Author_Name":"Oriol Vinyals","Author_ID":"None","Rel_Level":"medium"}}},{"Paper_Title":"Understanding deep learning requires rethinking generalization","Paper_Journal":"ArXiv","Journal_Level":"A","Paper_Year":"2016","Paper_Author":{"Chiyuan Zhang":{"Author_Email":"None","Author_Name":"Chiyuan Zhang","Author_ID":"None","Author_Level":"R1"},"Samy Bengio":{"Author_Email":"None","Author_Name":"Samy Bengio","Author_ID":"None","Author_Level":"R1"},"Moritz Hardt":{"Author_Email":"None","Author_Name":"Moritz Hardt","Author_ID":"None","Author_Level":"R1"}},"Paper_Abstract":"Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.","Rel_Keyword":[{"Keyword":"Neural Networks","Keyword_Domain":"None","Rel_Level":"weak"},{"Keyword":"Deep Learning","Keyword_Domain":"None","Rel_Level":"medium"}],"Rel_Friend":{"Benjamin Recht":{"Author_Email":"None","Author_Name":"Benjamin Recht","Author_ID":"None","Rel_Level":"medium"},"Oriol Vinyals":{"Author_Email":"None","Author_Name":"Oriol Vinyals","Author_ID":"None","Rel_Level":"medium"}}},{"Paper_Title":"Understanding deep learning requires rethinking generalization","Paper_Journal":"ArXiv","Journal_Level":"A","Paper_Year":"2016","Paper_Author":{"Chiyuan Zhang":{"Author_Email":"None","Author_Name":"Chiyuan Zhang","Author_ID":"None","Author_Level":"R1"},"Samy Bengio":{"Author_Email":"None","Author_Name":"Samy Bengio","Author_ID":"None","Author_Level":"R1"},"Moritz Hardt":{"Author_Email":"None","Author_Name":"Moritz Hardt","Author_ID":"None","Author_Level":"R1"}},"Paper_Abstract":"Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.","Rel_Keyword":[{"Keyword":"Neural Networks","Keyword_Domain":"None","Rel_Level":"weak"},{"Keyword":"Deep Learning","Keyword_Domain":"None","Rel_Level":"medium"}],"Rel_Friend":{"Benjamin Recht":{"Author_Email":"None","Author_Name":"Benjamin Recht","Author_ID":"None","Rel_Level":"medium"},"Oriol Vinyals":{"Author_Email":"None","Author_Name":"Oriol Vinyals","Author_ID":"None","Rel_Level":"medium"}}},{"Paper_Title":"Deep Learning","Paper_Journal":"Adaptive computation and machine learning","Journal_Level":"A","Paper_Year":"2015","Paper_Author":{"Ian J. Goodfellow":{"Author_Email":"None","Author_Name":"Ian J. Goodfellow","Author_ID":"None","Author_Level":"R1"},"Peter Johnson":{"Author_Email":"None","Author_Name":"Peter Johnson","Author_ID":"None","Author_Level":"R1"},"Yoshua Bengio":{"Author_Email":"None","Author_Name":"Yoshua Bengio","Author_ID":"None","Author_Level":"R1"}},"Paper_Abstract":"Overview of the tutorial • A brief history of deep learning. • How to learn multi-layer generative models of unlabelled data by learning one layer of features at a time. – What is really going on when we stack RBMs to form a deep belief net. • How to use generative models to make discriminative training methods work much better for classification and regression. • How to modify RBMs to deal with real-valued input.","Rel_Keyword":[{"Keyword":"Deep Neural Network","Keyword_Domain":"None","Rel_Level":"weak"},{"Keyword":"Supervised","Keyword_Domain":"None","Rel_Level":"medium"}],"Rel_Friend":{"Yann Lecun":{"Author_Email":"None","Author_Name":"Yann Lecun","Author_ID":"None","Rel_Level":"medium"},"Yee Whye Teh":{"Author_Email":"None","Author_Name":"Yee Whye Teh","Author_ID":"None","Rel_Level":"medium"}}},{"Paper_Title":"Deep Learning","Paper_Journal":"Adaptive computation and machine learning","Journal_Level":"A","Paper_Year":"2015","Paper_Author":{"Ian J. Goodfellow":{"Author_Email":"None","Author_Name":"Ian J. Goodfellow","Author_ID":"None","Author_Level":"R1"},"Peter Johnson":{"Author_Email":"None","Author_Name":"Peter Johnson","Author_ID":"None","Author_Level":"R1"},"Yoshua Bengio":{"Author_Email":"None","Author_Name":"Yoshua Bengio","Author_ID":"None","Author_Level":"R1"}},"Paper_Abstract":"Overview of the tutorial • A brief history of deep learning. • How to learn multi-layer generative models of unlabelled data by learning one layer of features at a time. – What is really going on when we stack RBMs to form a deep belief net. • How to use generative models to make discriminative training methods work much better for classification and regression. • How to modify RBMs to deal with real-valued input.","Rel_Keyword":[{"Keyword":"Deep Neural Network","Keyword_Domain":"None","Rel_Level":"weak"},{"Keyword":"Supervised","Keyword_Domain":"None","Rel_Level":"medium"}],"Rel_Friend":{"Yann Lecun":{"Author_Email":"None","Author_Name":"Yann Lecun","Author_ID":"None","Rel_Level":"medium"},"Yee Whye Teh":{"Author_Email":"None","Author_Name":"Yee Whye Teh","Author_ID":"None","Rel_Level":"medium"}}},{"Paper_Title":"Multimodal Deep Learning","Paper_Journal":"ICML","Journal_Level":"A","Paper_Year":"2011","Paper_Author":{"Andrew Y. Ng":{"Author_Email":"None","Author_Name":"Andrew Y. Ng","Author_ID":"None","Author_Level":"R1"},"Honglak Lee":{"Author_Email":"None","Author_Name":"Honglak Lee","Author_ID":"None","Author_Level":"R1"},"Juhan Nam":{"Author_Email":"None","Author_Name":"Juhan Nam","Author_ID":"None","Author_Level":"R1"}},"Paper_Abstract":"Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning.","Rel_Keyword":[{"Keyword":"Deep networks","Keyword_Domain":"None","Rel_Level":"weak"},{"Keyword":"Autoencoder","Keyword_Domain":"None","Rel_Level":"medium"}],"Rel_Friend":{"Mingyu Kim":{"Author_Email":"None","Author_Name":"Mingyu Kim","Author_ID":"None","Rel_Level":"medium"},"Aditya Khosla":{"Author_Email":"None","Author_Name":"Aditya Khosla","Author_ID":"None","Rel_Level":"medium"}}},{"Paper_Title":"Multimodal Deep Learning","Paper_Journal":"ICML","Journal_Level":"A","Paper_Year":"2011","Paper_Author":{"Andrew Y. Ng":{"Author_Email":"None","Author_Name":"Andrew Y. Ng","Author_ID":"None","Author_Level":"R1"},"Honglak Lee":{"Author_Email":"None","Author_Name":"Honglak Lee","Author_ID":"None","Author_Level":"R1"},"Juhan Nam":{"Author_Email":"None","Author_Name":"Juhan Nam","Author_ID":"None","Author_Level":"R1"}},"Paper_Abstract":"Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning.","Rel_Keyword":[{"Keyword":"Deep networks","Keyword_Domain":"None","Rel_Level":"weak"},{"Keyword":"Autoencoder","Keyword_Domain":"None","Rel_Level":"medium"}],"Rel_Friend":{"Mingyu Kim":{"Author_Email":"None","Author_Name":"Mingyu Kim","Author_ID":"None","Rel_Level":"medium"},"Aditya Khosla":{"Author_Email":"None","Author_Name":"Aditya Khosla","Author_ID":"None","Rel_Level":"medium"}}},{"Paper_Title":"A Fast Learning Algorithm for Deep Belief Nets","Paper_Journal":"Neural Computation","Journal_Level":"A","Paper_Year":"2006","Paper_Author":{"Yee Whye Teh":{"Author_Email":"None","Author_Name":"Yee Whye Teh","Author_ID":"None","Author_Level":"R1"},"Simon Osindero":{"Author_Email":"None","Author_Name":"Simon Osindero","Author_ID":"None","Author_Level":"R1"},"Geoffrey E. Hinton,":{"Author_Email":"None","Author_Name":"Geoffrey E. Hinton,","Author_ID":"None","Author_Level":"R1"}},"Paper_Abstract":"We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.","Rel_Keyword":[{"Keyword":"Boltzmann Machines RBMs","Keyword_Domain":"None","Rel_Level":"weak"},{"Keyword":"Deep Belief Networks DBN","Keyword_Domain":"None","Rel_Level":"medium"}],"Rel_Friend":{"Tapani Raiko":{"Author_Email":"None","Author_Name":"Tapani Raiko","Author_ID":"None","Rel_Level":"medium"},"Yann LeCun":{"Author_Email":"None","Author_Name":"Yann LeCun","Author_ID":"None","Rel_Level":"medium"}}},{"Paper_Title":"A Fast Learning Algorithm for Deep Belief Nets","Paper_Journal":"Neural Computation","Journal_Level":"A","Paper_Year":"2006","Paper_Author":{"Yee Whye Teh":{"Author_Email":"None","Author_Name":"Yee Whye Teh","Author_ID":"None","Author_Level":"R1"},"Simon Osindero":{"Author_Email":"None","Author_Name":"Simon Osindero","Author_ID":"None","Author_Level":"R1"},"Geoffrey E. Hinton,":{"Author_Email":"None","Author_Name":"Geoffrey E. Hinton,","Author_ID":"None","Author_Level":"R1"}},"Paper_Abstract":"We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.","Rel_Keyword":[{"Keyword":"Boltzmann Machines RBMs","Keyword_Domain":"None","Rel_Level":"weak"},{"Keyword":"Deep Belief Networks DBN","Keyword_Domain":"None","Rel_Level":"medium"}],"Rel_Friend":{"Tapani Raiko":{"Author_Email":"None","Author_Name":"Tapani Raiko","Author_ID":"None","Rel_Level":"medium"},"Yann LeCun":{"Author_Email":"None","Author_Name":"Yann LeCun","Author_ID":"None","Rel_Level":"medium"}}}]'
    var paper_data = $.parseJSON(paper_info); 

/*----------填充数据----------*/
    var li_str,link_str1,link_str2;
    $.each(paper_data,function(n,value){
    	link_str1 = '/paperDetail/?Paper_Title='+$.base64.encode(encodeURI(value.Paper_Title))+'&Paper_Journal='+$.base64.encode(encodeURI(value.Paper_Journal))+'&Paper_Year='+$.base64.encode(encodeURI(value.Paper_Year));
    	li_str = '<div class="re_block"><span class="se_year">' + value.Paper_Year + '</span><h1><a href=' + link_str1 +'>' + value.Paper_Title + '</a></h1><p class="se_author">';
    	$.each(value.Paper_Author,function(n,val){
    		link_str1 = '/authorDetail/?Author_Name='+$.base64.encode(encodeURI(val.Author_Name))+'&Author_Email='+$.base64.encode(encodeURI(val.Author_Email))+'&Author_ID='+$.base64.encode(encodeURI(val.Author_ID));
    		li_str += '<a href=' + link_str1 + '>' + n + '(' + val.Author_Level + ')</a>';
            li_str += ',';
    	});
        li_str = li_str.substring(0,li_str.length-2);
    	link_str1 = '/journaldetail/?Journal='+$.base64.encode(encodeURI(value.Paper_Journal));
    	li_str += '</p><p class="se_jour"><a href=' + link_str1 + '>' + value.Paper_Journal + '(' + value.Journal_Level + ')</a></p>';
        li_str += '<p class="se_abs"><span class="se_type" >摘要</span><span class="abs_content" title="'+ value.Paper_Abstract + '">' + value.Paper_Abstract + '</span></p><div class="se_relate" title="与我相关"><img src="./images/about.png">';
        link_str1 = '/keywordDetail/?Keyword='+$.base64.encode(encodeURI(value.Rel_Keyword[0].Keyword))+'&Keyword_Domain='+$.base64.encode(encodeURI(value.Rel_Keyword[0].Keyword_Domain));
        link_str2 = '/keywordDetail/?Keyword='+$.base64.encode(encodeURI(value.Rel_Keyword[1].Keyword))+'&Keyword_Domain='+$.base64.encode(encodeURI(value.Rel_Keyword[1].Keyword_Domain));
        li_str += '<p class="re_keyword">我的方向：<a href=' + link_str1 + '>' + value.Rel_Keyword[0].Keyword + '</a><span>' + value.Rel_Keyword[0].Rel_Level + '</span>,<a href=' + link_str2 + '>' + value.Rel_Keyword[1].Keyword + '</a><span>' + value.Rel_Keyword[1].Rel_Level + '</span></a></p>';
        li_str += '<p class="re_author">我的好友：';
        $.each(value.Rel_Friend,function(key,val){
            link_str1 = '/authorDetail/?Author_Name='+$.base64.encode(encodeURI(val.Author_Name))+'&Author_Email='+$.base64.encode(encodeURI(val.Author_Email))+'&Author_ID='+$.base64.encode(encodeURI(val.Author_ID));
            li_str += '<a href=' + link_str1 + '>' + key +'</a><span>' + val.Rel_Level + '</span>,';
        });
        li_str = li_str.substring(0,li_str.length-2);
        li_str += '</p></div></div>';
    	$('.result_box').append(li_str);
    });
    /*----------换一批----------*/
    var show_index = 0;
    function paper_change(){
        $('.re_block').hide();
        for (var i = show_index; i <= show_index + 5; i++) {
        	$('.re_block:eq(' + i + ')').show();
        }
    }
    paper_change();
    $('#change_word').click(function(){
        show_index = (show_index + 5)%paper_data.length;
        paper_change();
    });
    /*---------摘要收缩---------*/
    var abs_length = $('.re_block .se_abs').length;
    //var abs_list = $('.re_block .se_abs:eq(0) .se_content');
    for(var i = 0; i<abs_length; i++){
        var abs_obj = $('.re_block .se_abs:eq(' + i + ') .abs_content');
        var tit=abs_obj.attr('title');
        abs_obj.attr('abs',tit);
        abs_obj.attr('title','');
        if(tit.length > 420){
            abs_obj.text(tit.substring(0,420)+'...');
            abs_obj.parent().append('<span class="abs_more">(More)</span>');
        }
    }
    //click
    $('.se_abs .abs_more').click(function(){
        var abs_obj = $(this).parent().find('.abs_content');
        var tit = abs_obj.attr('abs');
        if(tit == abs_obj.text()){
            abs_obj.text(tit.substring(0,420)+'...');
            abs_obj.parent().find('.abs_more').text('(More)');
        }else{
            abs_obj.text(tit);
            abs_obj.parent().find('.abs_more').text('(Less)');
        }
    });
})